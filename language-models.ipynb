{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841b9a1f-16ff-4880-bed7-48e99b96cc3f",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "> Inspired by Jeff Dean's talk, [Exciting Trends in Machine Learning](https://youtu.be/oSCRZkSQ1CE).\n",
    "\n",
    "In this lesson, we'll apply what we know about neural networks toward the study of **language models**: probabilistic models of a natural (human) language, such as English. The goal of today is to become more conversational in language modeling techniques by studying recent natural language processing methods (rather than applications or implications). By the end of this lesson, students will be able to:\n",
    "\n",
    "- Explain how unigram and bigram language models determine the probability of the next word.\n",
    "- Explain how embedding spaces can provide a semantic, distributed representation for concepts.\n",
    "- Explain the benefits of self-attention mechanisms and hidden states in a RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45302cff-d6c7-4f56-b6a2-4f550d3da667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def clean(token: str, pattern: re.Pattern[str] = re.compile(r\"\\W+\")) -> str:\n",
    "    \"\"\"\n",
    "    Returns all the characters in the token lowercased and without matches to the given pattern.\n",
    "\n",
    "    >>> clean(\"Hello!\")\n",
    "    'hello'\n",
    "    \"\"\"\n",
    "    return pattern.sub(\"\", token.lower())\n",
    "\n",
    "\n",
    "def sample(frequencies: dict[Any, float], k: int = 1) -> list[Any]:\n",
    "    \"\"\"\n",
    "    Returns a list of k randomly sampled keys from the given frequencies with replacement.\n",
    "\n",
    "    >>> sample({\"test\": 1})\n",
    "    ['test']\n",
    "    >>> sample({\"test\": 1}, k=3)\n",
    "    ['test', 'test', 'test']\n",
    "    \"\"\"\n",
    "    return random.choices(list(frequencies), weights=frequencies.values(), k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f775cfe-5d86-4093-b3ed-56e2a3e22c93",
   "metadata": {},
   "source": [
    "## Statistical models\n",
    "\n",
    "In its simplest form, a language model is similar to the `Document` class that we defined in the *Search* assessment consisting of a term-frequency dictionary. A **unigram language model** guesses the next word based on the term-frequency dictionary alone. In the document, `doggos/doc1.txt`, each unique word appears once, so each word has a term frequency of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838336d8-0e16-4c15-a28c-77e01946afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = {\n",
    "    \"dogs\":     1,\n",
    "    \"are\":      1,\n",
    "    \"the\":      1,\n",
    "    \"greatest\": 1,\n",
    "    \"pets\":     1,\n",
    "}\n",
    "\n",
    "sample(terms, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83adf77a-8767-45d9-a210-af95de4913eb",
   "metadata": {},
   "source": [
    "Unigram models are simple but not particularly useful. For one, there's no notion of context: each word is sampled entirely independently from every other word. Large language models like ChatGPT combine learn from internet-scale training datasets and consider the preceding words (tokens) when determining the probability of the next word.\n",
    "\n",
    "A **bigram language model**, for instance, takes the immediately preceding word to determine the probabilities for the next word. We use `<s>` to indicate the start token. Modify this code snippet so that each `prev` term (key) has a dictionary consisting of the number of times the `curr` term appears afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ee841-8af5-46d3-a31b-8521643d2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = {}\n",
    "for filename in os.listdir(\"small_wiki\"):\n",
    "    if filename.endswith(\".html\"):\n",
    "        with open(os.path.join(\"small_wiki\", filename)) as f:\n",
    "            words = [\"<s>\"] + [clean(word) for word in f.read().split() if clean(word)]\n",
    "            for prev, curr in zip(words, words[1:]):\n",
    "                terms[prev] = curr\n",
    "\n",
    "terms[\"<s>\"] # Should be {\"metadata\": 70} to indicate all 70 documents start with \"metadata\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d222eac-cf09-4508-8e88-3e97d9958734",
   "metadata": {},
   "source": [
    "To generate a sequence of a given length, repeatedly use the `last` word to sample the next word and append it to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febb176-d612-4681-a051-4e629b3b4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 20\n",
    "result = \"what is the best\".split()\n",
    "for _ in range(n_words):\n",
    "    last = result[-1]\n",
    "    result += sample(terms[last])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ccc52-7fe9-4c85-bf14-7d83a18c56f4",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "Unigram and bigram language models are more generally known as **n-gram language models**. With larger context windows (n), n-gram models can produce more understandable results. But the approach has a fundamental limitation: it's sensitive to the exact words and the sequence they appeared in the training set.\n",
    "\n",
    "To address the first problem of learning word meaning, how might a computer even learn the meaning of a word? Strings in Python are sequences of characters where each character is just a number. Or, if you recall how we handled the city names \"NY\" and \"SF\" to predict the location of a home in model evaluation, strings could also be represented as \"dummy variables\" or boolean categories.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>beds</th>\n",
    "      <th>bath</th>\n",
    "      <th>year_built</th>\n",
    "      <th>sqft</th>\n",
    "      <th>price_per_sqft</th>\n",
    "      <th>elevation</th>\n",
    "      <th>city_NY</th>\n",
    "      <th>city_SF</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>2.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1960</td>\n",
    "      <td>1000</td>\n",
    "      <td>999</td>\n",
    "      <td>10</td>\n",
    "      <td>True</td>\n",
    "      <td>False</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>2.0</td>\n",
    "      <td>2.0</td>\n",
    "      <td>2006</td>\n",
    "      <td>1418</td>\n",
    "      <td>1939</td>\n",
    "      <td>0</td>\n",
    "      <td>True</td>\n",
    "      <td>False</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>2.0</td>\n",
    "      <td>2.0</td>\n",
    "      <td>1900</td>\n",
    "      <td>2150</td>\n",
    "      <td>628</td>\n",
    "      <td>9</td>\n",
    "      <td>True</td>\n",
    "      <td>False</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>1.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1903</td>\n",
    "      <td>500</td>\n",
    "      <td>1258</td>\n",
    "      <td>9</td>\n",
    "      <td>True</td>\n",
    "      <td>False</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>0.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1930</td>\n",
    "      <td>500</td>\n",
    "      <td>878</td>\n",
    "      <td>10</td>\n",
    "      <td>True</td>\n",
    "      <td>False</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>...</th>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>487</th>\n",
    "      <td>5.0</td>\n",
    "      <td>2.5</td>\n",
    "      <td>1890</td>\n",
    "      <td>3073</td>\n",
    "      <td>586</td>\n",
    "      <td>76</td>\n",
    "      <td>False</td>\n",
    "      <td>True</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>488</th>\n",
    "      <td>2.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1923</td>\n",
    "      <td>1045</td>\n",
    "      <td>665</td>\n",
    "      <td>106</td>\n",
    "      <td>False</td>\n",
    "      <td>True</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>489</th>\n",
    "      <td>3.0</td>\n",
    "      <td>2.0</td>\n",
    "      <td>1922</td>\n",
    "      <td>1483</td>\n",
    "      <td>1113</td>\n",
    "      <td>106</td>\n",
    "      <td>False</td>\n",
    "      <td>True</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>490</th>\n",
    "      <td>1.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>1983</td>\n",
    "      <td>850</td>\n",
    "      <td>764</td>\n",
    "      <td>163</td>\n",
    "      <td>False</td>\n",
    "      <td>True</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>491</th>\n",
    "      <td>3.0</td>\n",
    "      <td>2.0</td>\n",
    "      <td>1956</td>\n",
    "      <td>1305</td>\n",
    "      <td>762</td>\n",
    "      <td>216</td>\n",
    "      <td>False</td>\n",
    "      <td>True</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Word2vec refers to a technique that can learn a word embedding (semantic representation) by guessing how to fill in blank from the immediate context. Words that tend to appear in similar contexts probably have similar meanings, so an algorithm can learn the meaning of human words by examining a word's favorite neighbors. Words with similar neighbors should have similar representations. What word could appear in the following blank?\n",
    "\n",
    "> The city of _________ has an oceanic climate.\n",
    "\n",
    "This enables us to find synonyms as the [TensorFlow Embedding Projector](https://projector.tensorflow.org/) shows. But the authors also pointed out some interesting [examples of the learned relationships](https://arxiv.org/pdf/1301.3781.pdf#page=10), such as how, in the embedding space, `Paris - France + Italy = Rome`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00430943-4ff0-4cb5-81f0-71609f12e7ab",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "Although word embeddings produce a semantic representation for the meaning of words, there still remains a challenge of how we might combine these word embeddings to form meaningful sentences. How might we use machine learning to combine word embeddings in a way that is sensitive to context?\n",
    "\n",
    "Earlier, we learned two ways that neural networks could be used to classify handwritten digit images. But both approaches (scikit-learn `MLPClassifier` and Keras `Conv2D`) involved learning weights and biases only from the input pixel values. In language modeling where the possible sequences of words are infinite, it becomes much harder to train neural network weights and biases to directly handle every possibility.\n",
    "\n",
    "**Recurrent neural networks** (RNNs) represent a different way of organizing a neural network by calculating the output of a neuron taking into account not only the inputs but also a *hidden state* that represents previously-generated outputs. Unlike *hidden layers* in a neural network, **hidden states** provide additional information about previously-generated steps to the current step. In other words, a RNN learns to predict the next word based on the current input as well as information obtained from previous words.\n",
    "\n",
    "For example, we can use recurrent neural networks for language modeling by considering how they might generate a response sequence. Given an input `X` of one or more words, a recurrent neural network generates an output `O` one word at a time while considering the hidden state `V`.\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The Keras code examples includes a sample for [character-level text generation with LSTM](https://keras.io/examples/generative/lstm_character_level_text_generation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1dfd87-cf1d-4706-98ed-cba9c25cab38",
   "metadata": {},
   "source": [
    "### Sequence-to-sequence framework\n",
    "\n",
    "The **seq2seq** framework utilizes 2 RNNs to implement \"sequence-to-sequence\" tasks:\n",
    "\n",
    "- An **encoder** that learns a machine representation for an input sequence, or *context*.\n",
    "- A **decoder** that can take a machine representation (context) and decode it to a target sequence.\n",
    "\n",
    "Originally, these models were used for machine translation, so the problem was framed as [reading an input sentence \"ABC\" and producing \"WXYZ\" as output](https://arxiv.org/pdf/1409.3215.pdf#page=2). By changing the training set to give the encoder context and have the decoder predict the expected reply, [the seq2seq framework can be used to model conversations](https://arxiv.org/pdf/1506.05869.pdf#page=2). How does this framework differ from using a single recurrent neural network for language modeling?\n",
    "\n",
    "The Keras code examples also includes a sample for [character-level sequence-to-sequence modeling](https://keras.io/examples/nlp/lstm_seq2seq/). For word-level tasks, we can use word embeddings to provide more information to the model about the meaning of each word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcc7e430-b1ac-4681-bd67-f0e0636e7e4a",
   "metadata": {},
   "source": [
    "## Transformer architecture\n",
    "\n",
    "With more data, the seq2seq approach can produce good results, but at a high computational cost because the hidden state needs to be updated on each step. Recurrent neural networks end up spending significant amounts of resources computing hidden states sequentially.\n",
    "\n",
    "The **transformer architecture** addresses this by throwing out the RNN and instead utilizing a mechanism called *self-attention* to learn context without relying on hidden states. The goal of **self-attention** is to identify associations or relationships between different words in a sequence. Consider the sentence:\n",
    "\n",
    "> The animal didn't cross the street because it was too tired\n",
    "\n",
    "What does \"it\" refer to—the animal or the street? Let's read about why this makes a difference in translation on the [Google Research blog](https://blog.research.google/2017/08/transformer-novel-neural-network.html) and study the [Tensor2Tensor Intro notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb) to see for ourselves how attention heads learn different relationships between words.\n",
    "\n",
    "Although the transformer architecture was not the first to introduce the idea of an attention mechanism, it has since become the most popular way to not only use attention but to define language models in general."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
