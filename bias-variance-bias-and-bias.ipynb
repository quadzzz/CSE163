{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcb95b9-aab2-404c-8572-cff796491ec3",
   "metadata": {},
   "source": [
    "# Bias, Variance, Bias, and Bias\n",
    "\n",
    "When we choose between different machine learning models, we also encode what we expect from data in the real-world. In this lesson, we'll expand on model evaluation to focus on developing a theory for \"bias\" situated in computational and sociological perspectives.\n",
    "\n",
    "- Explain the bias–variance tradeoff for a given model.\n",
    "- Identify a data imbalance from a data visualization.\n",
    "- Identify the factors that determine a machine learning model's predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122a337-a22b-4144-bad4-77b3d32a9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interactive slider widget\n",
    "from ipywidgets import interact\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720ae4a-8960-4671-8f49-40282827a36d",
   "metadata": {},
   "source": [
    "Like before, we'll load in our sensor data but this time focus only on using the PurpleAir sensor (PAS) measurements to predict the EPA Air Quality Sensor (AQS) measurements using a `DecisionTreeRegressor` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f27a9-ae63-4d23-be6f-cf77a2c8c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data = pd.read_csv(\"sensor_data.csv\")\n",
    "X = sensor_data[[\"PAS\"]]\n",
    "y = sensor_data[\"AQS\"]\n",
    "# Create a demonstration dataset that counts from 0 to the max PAS value\n",
    "X_plot = pd.DataFrame(np.arange(sensor_data[\"PAS\"].max()), columns=[\"PAS\"])\n",
    "\n",
    "sensor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f460062-43a8-47e0-aecc-2f2c179d4c6f",
   "metadata": {},
   "source": [
    "## Bias–variance tradeoff\n",
    "\n",
    "During model evaluation, we used `GridSearchCV` for 5-fold cross-validation to select the `max_depth` hyperparameter value. We can plot the validation errors as a line plot showing the relationship between `max_depth` and the validation score (negative RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504714e5-dd3b-4dc5-b29e-803595001bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = list(range(1, 13))\n",
    "\n",
    "# Grid search cross-validation to tune the max_depth hyperparameter using RMSE loss metric\n",
    "search = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    param_grid={\"max_depth\": max_depths},\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=1,\n",
    ")\n",
    "search.fit(X, y)\n",
    "# Print the best score and best estimator at the end of hyperparameter search\n",
    "reg = search.best_estimator_\n",
    "print(\"Best model:\", reg)\n",
    "print(\"Mean score:\", search.best_score_)\n",
    "\n",
    "# Load results of cross-validation into a DataFrame\n",
    "cv_results = pd.DataFrame(search.cv_results_)[[\n",
    "    \"param_max_depth\",\n",
    "    \"split0_test_score\",\n",
    "    \"split1_test_score\",\n",
    "    \"split2_test_score\",\n",
    "    \"split3_test_score\",\n",
    "    \"split4_test_score\",\n",
    "    \"mean_test_score\",\n",
    "]]\n",
    "cv_results = cv_results.melt(id_vars=[\"param_max_depth\"]).set_index(\"param_max_depth\") * (-1)\n",
    "# Plot the validation scores with confidence intervals\n",
    "grid = sns.relplot(cv_results, x=\"param_max_depth\", y=\"value\", kind=\"line\")\n",
    "grid.set(title=\"DecisionTreeRegressor CV error\", xlabel=\"Max depth\", ylabel=\"Validation RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1047d-48c9-4d60-8284-784acff7be8f",
   "metadata": {},
   "source": [
    "A very complex model such as a `DecisionTreeRegressor(max_depth=12)` is highly sensitive to variance in the training dataset: each time we sample a new training dataset, the decision tree learns the noise in the sample rather than the trend. A model has **high variance** if it is very sensitive to different training dataset samples.\n",
    "\n",
    "On the other hand, a very simple model such as a `DecisionTreeRegressor(max_depth=1)` won't work well in the real-world because it hasn't learned enough about the training dataset. A model has **high bias** if it misses important relationships between features and labels. Ideally, we want to choose a model that optimizes for this **bias–variance tradeoff** using, for example, cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a80bf-821f-4306-8136-7474d2e309a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly re-sample a training dataset and a testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "@interact(max_depth=(1, 12, 1))\n",
    "def lmplot_compare(max_depth=12):\n",
    "    grid = sns.lmplot(sensor_data, x=\"PAS\", y=\"AQS\", ci=None)\n",
    "    reg = DecisionTreeRegressor(max_depth=max_depth).fit(X_train, y_train)\n",
    "    grid.facet_axis(0, 0).plot(X_plot, reg.predict(X_plot), color=\"orange\", linewidth=3)\n",
    "    train_error = mean_squared_error(y_train, reg.predict(X_train), squared=False)\n",
    "    test_error = mean_squared_error(y_test, reg.predict(X_test), squared=False)\n",
    "    grid.set(title=f\"Training error = {train_error:.2f}, Testing error = {test_error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11092f-61f7-4b7f-8990-ae8ef4642a12",
   "metadata": {},
   "source": [
    "## Imbalanced data\n",
    "\n",
    "But cross-validation doesn't always solve all our problems: cross-validation depends on our dataset, so it can reproduce the *selection bias* inherent in the dataset. In the sensor dataset, there are only 5 observations with AQS higher than 50. But if our goal is to produce a reliable prediction for real-time air quality from PurpleAir sensor measurements, our model should focus its accuracy on higher values rather than lower values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3b168-43d1-4fc3-b36d-336c76b66a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data[sensor_data[\"AQS\"] > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bbf96-8212-4858-ad10-6c55143bc2eb",
   "metadata": {},
   "source": [
    "## Algorithmic bias\n",
    "\n",
    "In this case, the underrepresentation of larger PAS and AQS values leads to systematic and repeatable errors that would appear anytime we run into larger values. **Algorithmic bias** occurs when a machine learning model produces categorically-unfair outcomes: in this case, our decision tree is more effective at working with small values than it is at large values.\n",
    "\n",
    "Imbalanced data can be one reason why algorithmic bias occurs, but it's only one part of a machine learning model:\n",
    "\n",
    "1. **Training dataset** implicated by data imbalances and the surrounding data setting.\n",
    "1. **Machine learning algorithm** that define the kinds of patterns that the model can learn.\n",
    "1. **Evaluation metrics** that provide a target for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43550ab-4012-401c-a479-1a18061ea232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the tidy (long-form) dataframe for Min degree = associate's\n",
    "index = pd.MultiIndex.from_product([\n",
    "    [\"White\", \"Black\", \"Hispanic\", \"Asian\", \"Pacific Islander\", \"American Indian/Alaska Native\"],\n",
    "    range(2009, 2019),\n",
    "], names=[\"Race\", \"Year\"])\n",
    "data = pd.DataFrame([\n",
    "    47.1, 48.9, 50.1, 49.9, 51.0, 51.9, 54.0, 54.3, 53.5, 53.6,\n",
    "    27.8, 29.4, 29.8, 31.6, 29.5, 32.0, 31.1, 31.7, 32.7, 32.6,\n",
    "    18.4, 20.5, 20.6, 22.7, 23.1, 23.4, 25.7, 27.0, 27.7, 30.5,\n",
    "    66.7, 63.4, 64.6, 68.3, 67.2, 70.3, 71.7, 71.5, 69.9, 75.5,\n",
    "    20.9, 22.0, 39.7, 32.4, 37.3, float(\"nan\"), 24.9, 28.6, 35.8, 22.6,\n",
    "    20.8, 28.9, 25.0, 23.6, 26.3, 18.2, 22.3, 16.5, 27.1, 24.4\n",
    "], index=index, columns=[\"Percentage\"])\n",
    "\n",
    "# Recreate the line plot comparing educational attainment by race\n",
    "grid = sns.relplot(data, x=\"Year\", y=\"Percentage\", hue=\"Race\", kind=\"line\")\n",
    "grid.set(title=\"Asian educational attainment reaches new heights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fff931-c232-4903-836d-10349694a120",
   "metadata": {},
   "source": [
    "## Optional: Creating custom comparison plots\n",
    "\n",
    "The `interact` slider widget that we can use to explore the plots above is great, but doesn't work well for static (non-interactive) presentation formats like PDF. Instead, we can make a larger comparison plot by manually creating a `FacetGrid` and calling the `map` or `map_dataframe` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22a369-7b22-4722-b2b3-63257714680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decisions(data, reg_cls, *args, **kwargs):\n",
    "    \"\"\"Trains a regression model for the given data hyperparameters and plots predictions.\"\"\"\n",
    "    hyperparameters = data.iloc[0, :]\n",
    "    reg = reg_cls(**hyperparameters).fit(X_train, y_train)\n",
    "    plt.plot(X_plot, reg.predict(X_plot), **kwargs)\n",
    "\n",
    "\n",
    "# Manually create a 4-column FacetGrid to compare different hyperparameter values\n",
    "grid = sns.FacetGrid(pd.DataFrame(max_depths, columns=[\"max_depth\"]), col=\"max_depth\", col_wrap=4)\n",
    "\n",
    "# First, plot a linear regression model in the background\n",
    "grid.map(sns.regplot, data=sensor_data, x=\"PAS\", y=\"AQS\", ci=None, color=\"lightgray\")\n",
    "\n",
    "# Then, plot the specified regression algorithm for each hyperparameter value\n",
    "grid.map_dataframe(plot_decisions, reg_cls=DecisionTreeRegressor, color=\"orange\", linewidth=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
